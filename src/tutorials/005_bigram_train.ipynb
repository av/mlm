{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "max_iters = 3_000\n",
    "eval_interval = 300\n",
    "learning_rate = 1e-2\n",
    "eval_iters = 200\n",
    "train_split = 0.9\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('/data/tinyshakespeare.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: map string to char ids\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: map integer ids to a string\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "n = int(train_split * len(data)) # 90% training, 10% validation\n",
    "train_data, validation_data = data[:n], data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "  # batch of data of inputs x and targets y\n",
    "  data = train_data if split == 'train' else validation_data\n",
    "  # array(batch_size) of random offsets within data\n",
    "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "\n",
    "  # Context and target sequences\n",
    "  # ? Why a single sequence is not used?\n",
    "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "  x, y = x.to(device), y.to(device)\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "  out = {}\n",
    "  model.eval()\n",
    "\n",
    "  for split in ['train', 'validation']:\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "      X, Y = get_batch(split)\n",
    "      logits, loss = model(X, Y)\n",
    "      losses[k] = loss.item()\n",
    "\n",
    "    out[split] = losses.mean()\n",
    "\n",
    "  model.train()\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # Video says: each token directly reads off the logits for the next token\n",
    "    # from the lookup table.\n",
    "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "  def forward(self, idx, targets=None):\n",
    "    # idx and targets are (B, T) tensors of integers, Batch, Time\n",
    "    # Token embedding table will return shape of (Batch, Time, Channel)\n",
    "    # Batch - number of sequences\n",
    "    # Time - number of tokens in each sequence\n",
    "    # Channel - Embeddings vector for a token?\n",
    "    logits = self.token_embedding_table(idx)\n",
    "\n",
    "    if targets is None:\n",
    "      return logits, None\n",
    "\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.view(B*T, C)\n",
    "    targets = targets.view(B*T)\n",
    "\n",
    "    # Cross entropy expects input to be in a 2D tensor of shape (Batch*Time, Channel)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "\n",
    "  def generate(self, idx, max_new_tokens):\n",
    "    # idx is a (Batch, Time) tensor of integers, representing current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      # compute the predictions\n",
    "      logits, loss = self(idx) # (B, T, C)\n",
    "\n",
    "      # -1 makes very little sense for a bigram model,\n",
    "      # as we're essentially throwing away everything except the very last token in a batch\n",
    "      # to make our prediction.\n",
    "      # This is done in such a way only to allow for easier transition to an N-gram model later.\n",
    "      logits = logits[:, -1, :] # (B, C)\n",
    "\n",
    "      # Probabilities from logits\n",
    "      # Softmax ~ [1, 2, 3, 4]\n",
    "      probs = F.softmax(logits, dim=-1)\n",
    "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "      # append to the currently running context\n",
    "      idx = torch.cat((idx, idx_next), dim=1) # (B, T + 1)\n",
    "    return idx\n",
    "\n",
    "model = BigramLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Train loss: 4.762716770172119, Validation loss: 4.763288974761963\n",
      "Iter 300, Train loss: 2.8414525985717773, Validation loss: 2.8634636402130127\n",
      "Iter 600, Train loss: 2.5515248775482178, Validation loss: 2.5881149768829346\n",
      "Iter 900, Train loss: 2.4970433712005615, Validation loss: 2.5305380821228027\n",
      "Iter 1200, Train loss: 2.490762233734131, Validation loss: 2.5049004554748535\n",
      "Iter 1500, Train loss: 2.4716250896453857, Validation loss: 2.5027894973754883\n",
      "Iter 1800, Train loss: 2.45572566986084, Validation loss: 2.4971234798431396\n",
      "Iter 2100, Train loss: 2.4724764823913574, Validation loss: 2.4942057132720947\n",
      "Iter 2400, Train loss: 2.463688611984253, Validation loss: 2.4955973625183105\n",
      "Iter 2700, Train loss: 2.4643714427948, Validation loss: 2.489426374435425\n",
      "Iter 3000, Train loss: 2.4624266624450684, Validation loss: 2.493394374847412\n",
      "Iter 3300, Train loss: 2.460221290588379, Validation loss: 2.493234634399414\n",
      "Iter 3600, Train loss: 2.454041004180908, Validation loss: 2.485175132751465\n",
      "Iter 3900, Train loss: 2.464303970336914, Validation loss: 2.4900646209716797\n",
      "Iter 4200, Train loss: 2.4627771377563477, Validation loss: 2.4850013256073\n",
      "Iter 4500, Train loss: 2.458220958709717, Validation loss: 2.4824647903442383\n",
      "Iter 4800, Train loss: 2.4574739933013916, Validation loss: 2.4920222759246826\n",
      "Iter 5100, Train loss: 2.447075605392456, Validation loss: 2.488405466079712\n",
      "Iter 5400, Train loss: 2.456052303314209, Validation loss: 2.4969334602355957\n",
      "Iter 5700, Train loss: 2.460055112838745, Validation loss: 2.4853224754333496\n",
      "Iter 6000, Train loss: 2.4642865657806396, Validation loss: 2.4861738681793213\n",
      "Iter 6300, Train loss: 2.453505516052246, Validation loss: 2.4853768348693848\n",
      "Iter 6600, Train loss: 2.4558298587799072, Validation loss: 2.491156816482544\n",
      "Iter 6900, Train loss: 2.4529504776000977, Validation loss: 2.4888570308685303\n",
      "Iter 7200, Train loss: 2.4629170894622803, Validation loss: 2.4873650074005127\n",
      "Iter 7500, Train loss: 2.4573984146118164, Validation loss: 2.4876835346221924\n",
      "Iter 7800, Train loss: 2.448779821395874, Validation loss: 2.493211030960083\n",
      "Iter 8100, Train loss: 2.457484245300293, Validation loss: 2.480038642883301\n",
      "Iter 8400, Train loss: 2.4631261825561523, Validation loss: 2.4907238483428955\n",
      "Iter 8700, Train loss: 2.459165096282959, Validation loss: 2.4919989109039307\n",
      "Iter 9000, Train loss: 2.453723669052124, Validation loss: 2.494290590286255\n",
      "Iter 9300, Train loss: 2.464322090148926, Validation loss: 2.482966661453247\n",
      "Iter 9600, Train loss: 2.4621241092681885, Validation loss: 2.4833948612213135\n",
      "Iter 9900, Train loss: 2.442791223526001, Validation loss: 2.492373466491699\n",
      "Iter 10200, Train loss: 2.466914653778076, Validation loss: 2.491178512573242\n",
      "Iter 10500, Train loss: 2.454838752746582, Validation loss: 2.4886844158172607\n",
      "Iter 10800, Train loss: 2.4551846981048584, Validation loss: 2.4921202659606934\n",
      "Iter 11100, Train loss: 2.4513893127441406, Validation loss: 2.489835262298584\n",
      "Iter 11400, Train loss: 2.4601638317108154, Validation loss: 2.4915168285369873\n",
      "Iter 11700, Train loss: 2.4579086303710938, Validation loss: 2.488065719604492\n",
      "Iter 12000, Train loss: 2.465582847595215, Validation loss: 2.5060253143310547\n",
      "Iter 12300, Train loss: 2.4482908248901367, Validation loss: 2.48835825920105\n",
      "Iter 12600, Train loss: 2.4623119831085205, Validation loss: 2.4930245876312256\n",
      "Iter 12900, Train loss: 2.454019546508789, Validation loss: 2.4987804889678955\n",
      "Iter 13200, Train loss: 2.4583427906036377, Validation loss: 2.4810194969177246\n",
      "Iter 13500, Train loss: 2.454486131668091, Validation loss: 2.492952585220337\n",
      "Iter 13800, Train loss: 2.454712152481079, Validation loss: 2.4862771034240723\n",
      "Iter 14100, Train loss: 2.46085786819458, Validation loss: 2.494119644165039\n",
      "Iter 14400, Train loss: 2.4672417640686035, Validation loss: 2.482473373413086\n",
      "Iter 14700, Train loss: 2.45803165435791, Validation loss: 2.4943153858184814\n",
      "Iter 15000, Train loss: 2.456803560256958, Validation loss: 2.502977132797241\n",
      "Iter 15300, Train loss: 2.4571797847747803, Validation loss: 2.492227077484131\n",
      "Iter 15600, Train loss: 2.451087713241577, Validation loss: 2.4814374446868896\n",
      "Iter 15900, Train loss: 2.4628827571868896, Validation loss: 2.5036396980285645\n",
      "Iter 16200, Train loss: 2.453242540359497, Validation loss: 2.4818358421325684\n",
      "Iter 16500, Train loss: 2.457446336746216, Validation loss: 2.49804425239563\n",
      "Iter 16800, Train loss: 2.4524405002593994, Validation loss: 2.4989094734191895\n",
      "Iter 17100, Train loss: 2.4536993503570557, Validation loss: 2.4918739795684814\n",
      "Iter 17400, Train loss: 2.4507017135620117, Validation loss: 2.483912706375122\n",
      "Iter 17700, Train loss: 2.4607269763946533, Validation loss: 2.49269962310791\n",
      "Iter 18000, Train loss: 2.461636543273926, Validation loss: 2.4847960472106934\n",
      "Iter 18300, Train loss: 2.457613945007324, Validation loss: 2.4966602325439453\n",
      "Iter 18600, Train loss: 2.459003448486328, Validation loss: 2.485642910003662\n",
      "Iter 18900, Train loss: 2.4600555896759033, Validation loss: 2.508835554122925\n",
      "Iter 19200, Train loss: 2.460141658782959, Validation loss: 2.487407922744751\n",
      "Iter 19500, Train loss: 2.4565813541412354, Validation loss: 2.49230694770813\n",
      "Iter 19800, Train loss: 2.45223331451416, Validation loss: 2.495574951171875\n",
      "Iter 20100, Train loss: 2.456310272216797, Validation loss: 2.4836785793304443\n",
      "Iter 20400, Train loss: 2.465846061706543, Validation loss: 2.49212646484375\n",
      "Iter 20700, Train loss: 2.461754560470581, Validation loss: 2.4999098777770996\n",
      "Iter 21000, Train loss: 2.461148977279663, Validation loss: 2.503312587738037\n",
      "Iter 21300, Train loss: 2.457855224609375, Validation loss: 2.4841649532318115\n",
      "Iter 21600, Train loss: 2.4468495845794678, Validation loss: 2.4957520961761475\n",
      "Iter 21900, Train loss: 2.4506452083587646, Validation loss: 2.4978675842285156\n",
      "Iter 22200, Train loss: 2.4525065422058105, Validation loss: 2.5055320262908936\n",
      "Iter 22500, Train loss: 2.463728427886963, Validation loss: 2.502723217010498\n",
      "Iter 22800, Train loss: 2.446751117706299, Validation loss: 2.485203504562378\n",
      "Iter 23100, Train loss: 2.466574192047119, Validation loss: 2.4856479167938232\n",
      "Iter 23400, Train loss: 2.4546897411346436, Validation loss: 2.4902892112731934\n",
      "Iter 23700, Train loss: 2.457092046737671, Validation loss: 2.4815685749053955\n",
      "Iter 24000, Train loss: 2.4510011672973633, Validation loss: 2.4933884143829346\n",
      "Iter 24300, Train loss: 2.457113742828369, Validation loss: 2.4939539432525635\n",
      "Iter 24600, Train loss: 2.45924711227417, Validation loss: 2.486632823944092\n",
      "Iter 24900, Train loss: 2.452176809310913, Validation loss: 2.490464687347412\n",
      "Iter 25200, Train loss: 2.4489378929138184, Validation loss: 2.4983766078948975\n",
      "Iter 25500, Train loss: 2.4480674266815186, Validation loss: 2.490189552307129\n",
      "Iter 25800, Train loss: 2.458245277404785, Validation loss: 2.497243881225586\n",
      "Iter 26100, Train loss: 2.4468772411346436, Validation loss: 2.4924826622009277\n",
      "Iter 26400, Train loss: 2.458822250366211, Validation loss: 2.4958999156951904\n",
      "Iter 26700, Train loss: 2.4562315940856934, Validation loss: 2.4926657676696777\n",
      "Iter 27000, Train loss: 2.4629311561584473, Validation loss: 2.4938271045684814\n",
      "Iter 27300, Train loss: 2.4597432613372803, Validation loss: 2.4839706420898438\n",
      "Iter 27600, Train loss: 2.457474708557129, Validation loss: 2.494725227355957\n",
      "Iter 27900, Train loss: 2.4506280422210693, Validation loss: 2.495392084121704\n",
      "Iter 28200, Train loss: 2.4584977626800537, Validation loss: 2.491853713989258\n",
      "Iter 28500, Train loss: 2.462644338607788, Validation loss: 2.497147798538208\n",
      "Iter 28800, Train loss: 2.4502902030944824, Validation loss: 2.5045056343078613\n",
      "Iter 29100, Train loss: 2.4545230865478516, Validation loss: 2.4885847568511963\n",
      "Iter 29400, Train loss: 2.4525225162506104, Validation loss: 2.48933744430542\n",
      "Iter 29700, Train loss: 2.4600090980529785, Validation loss: 2.4997735023498535\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "  if iter % eval_interval == 0:\n",
    "    losses = estimate_loss()\n",
    "    print(f'Iter {iter}, Train loss: {losses[\"train\"]}, Validation loss: {losses[\"validation\"]}')\n",
    "\n",
    "  # Sample batch of data\n",
    "  xb, yb = get_batch('train')\n",
    "\n",
    "  # Eval\n",
    "  logits, loss = model(xb, yb)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ONS:\n",
      "GSored ve y ube I thellurs:\n",
      "DWAncas syshapy d tr wn, thirdsto;\n",
      "PRI d:\n",
      "ARard.\n",
      "Pr have S yorves,\n",
      "HET:\n",
      "I's, mepre be bode ave kes,\n",
      "Whingerer t, ardset\n",
      "Thin plun, he, ars len; mecowirund ENThayoncerkie buse myo me hot. I'sperifive l I:\n",
      "We!\n",
      "I heree yos\n",
      "\n",
      "Theme!'l boanf end\n",
      "Weit I tesueoomollcthe spt t:\n",
      "I myscer oucond andrinu, eromerint. wh byo: H:\n",
      "UCI aned n amerd icknen. bonem, ws, h tonghot nd, w, fieacou t meer, pele mabry f ch by amach's;\n",
      "Andfube s oumist quo me omorn-s\n",
      "O:\n",
      "Cllatrnco F owind \n"
     ]
    }
   ],
   "source": [
    "ctx = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "batch = model.generate(ctx, max_new_tokens=500)\n",
    "logits = batch[0].tolist()\n",
    "\n",
    "print(decode(logits))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
