{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bc026d",
   "metadata": {},
   "source": [
    "# MIRAS Simple Runner\n",
    "\n",
    "Minimal 4-cell notebook to run MIRAS language model:\n",
    "1. **Config** - Set hyperparameters\n",
    "2. **Init** - Load data and build model\n",
    "3. **Train** - Train the model\n",
    "4. **Generate** - Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf44818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "\n",
    "# Dataset\n",
    "DATASET_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "# Model architecture\n",
    "N_EMBD = 128          # Embedding dimension\n",
    "N_LAYERS = 2          # Number of MIRAS blocks\n",
    "BLOCK_SIZE = 64       # Context length\n",
    "MEMORY_TYPE = 'deep'  # 'linear' or 'deep'\n",
    "ATTENTIONAL_BIAS = 'l2'  # 'l2', 'lp', or 'huber'\n",
    "RETENTION = 'l2'      # 'l2', 'kl', or 'elastic'\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "MAX_ITERS = 5000\n",
    "EVAL_INTERVAL = 500\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Generation\n",
    "MAX_NEW_TOKENS = 200\n",
    "TEMPERATURE = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b184030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INIT ===\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "from typing import Optional, Tuple, Callable\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Data ---\n",
    "text = requests.get(DATASET_URL).text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data, val_data = data[:n], data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    d = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(d) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([d[i:i+BLOCK_SIZE] for i in ix]).to(device)\n",
    "    y = torch.stack([d[i+1:i+BLOCK_SIZE+1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}, Train tokens: {len(train_data):,}\")\n",
    "\n",
    "# --- Model Components ---\n",
    "def l2_loss(pred, target):\n",
    "    return 0.5 * ((pred - target) ** 2).sum(dim=-1)\n",
    "\n",
    "def lp_loss(pred, target, p=3):\n",
    "    return (torch.abs(pred - target) ** p).sum(dim=-1)\n",
    "\n",
    "def huber_loss(pred, target, delta):\n",
    "    diff = pred - target\n",
    "    abs_diff = torch.abs(diff)\n",
    "    return torch.where(abs_diff <= delta, 0.5 * diff ** 2, delta * (abs_diff - 0.5 * delta)).sum(dim=-1)\n",
    "\n",
    "def l2_retention_update(W, grad, alpha, eta):\n",
    "    return alpha * W - eta * grad\n",
    "\n",
    "def kl_retention_update(log_W, grad, alpha, eta, c=1.0):\n",
    "    log_W_new = alpha * log_W - eta * grad\n",
    "    return log_W_new, c * F.softmax(log_W_new, dim=-1)\n",
    "\n",
    "def elastic_net_update(W, grad, lambda_decay, zeta_lr, gamma_l1):\n",
    "    z = lambda_decay * W - zeta_lr * grad\n",
    "    return torch.sign(z) * F.relu(torch.abs(z) - gamma_l1)\n",
    "\n",
    "class KeyValueProjection(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W_K(x), self.W_V(x), self.W_Q(x)\n",
    "\n",
    "class MIRASLayer(nn.Module):\n",
    "    def __init__(self, d, memory_type='deep', attentional_bias='l2', retention='l2', expansion=4, p=3, q=4):\n",
    "        super().__init__()\n",
    "        self.d, self.memory_type, self.attentional_bias, self.retention = d, memory_type, attentional_bias, retention\n",
    "        self.p, self.q = p, q\n",
    "        self.kv_proj = KeyValueProjection(d, d)\n",
    "\n",
    "        if memory_type == 'linear':\n",
    "            self.register_buffer('M_init', torch.zeros(d, d))\n",
    "        else:\n",
    "            self.W1_init = nn.Parameter(torch.randn(d, d * expansion) * 0.02)\n",
    "            self.W2_init = nn.Parameter(torch.randn(d * expansion, d) * 0.02)\n",
    "            self.ln = nn.LayerNorm(d)\n",
    "\n",
    "        if attentional_bias == 'huber':\n",
    "            self.delta_proj = nn.Linear(d, 1)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(1) * 0.9)\n",
    "        self.eta = nn.Parameter(torch.ones(1) * 0.1)\n",
    "        if retention == 'kl':\n",
    "            self.c = nn.Parameter(torch.ones(1))\n",
    "        if retention == 'elastic':\n",
    "            self.gamma = nn.Parameter(torch.ones(1) * 0.01)\n",
    "\n",
    "    def memory_forward_deep(self, x, W1, W2):\n",
    "        h = F.gelu(x @ W2.transpose(-2, -1))\n",
    "        return x + self.ln(h @ W1.transpose(-2, -1))\n",
    "\n",
    "    def get_loss(self, pred, target, x_t=None):\n",
    "        if self.attentional_bias == 'l2':\n",
    "            return l2_loss(pred, target).sum()\n",
    "        elif self.attentional_bias == 'lp':\n",
    "            return lp_loss(pred, target, self.p).sum()\n",
    "        else:\n",
    "            return huber_loss(pred, target, F.softplus(self.delta_proj(x_t))).sum()\n",
    "\n",
    "    def apply_retention(self, W, grad, log_W=None):\n",
    "        alpha, eta = torch.sigmoid(self.alpha), F.softplus(self.eta)\n",
    "        if self.retention == 'l2':\n",
    "            return l2_retention_update(W, grad, alpha, eta), None\n",
    "        elif self.retention == 'kl':\n",
    "            log_W = log_W if log_W is not None else torch.log(W.clamp(min=1e-10))\n",
    "            log_W_new, W_new = kl_retention_update(log_W, grad, alpha, eta, self.c)\n",
    "            return W_new, log_W_new\n",
    "        else:\n",
    "            return elastic_net_update(W, grad, alpha, eta, self.gamma), None\n",
    "\n",
    "    def forward(self, x):\n",
    "        k, v, q = self.kv_proj(x)\n",
    "        B, T, D = k.shape\n",
    "        outputs = []\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            if self.memory_type == 'linear':\n",
    "                M = self.M_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "                for t in range(T):\n",
    "                    k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "                    M_leaf = M.detach().requires_grad_(True)\n",
    "                    pred = torch.einsum('bde,be->bd', M_leaf, k_t)\n",
    "                    loss = self.get_loss(pred, v_t, x[:, t] if self.attentional_bias == 'huber' else None)\n",
    "                    grad = torch.autograd.grad(loss, M_leaf)[0]\n",
    "                    M, _ = self.apply_retention(M, grad)\n",
    "                    outputs.append(torch.einsum('bde,be->bd', M, q_t))\n",
    "            else:\n",
    "                W1 = self.W1_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "                W2 = self.W2_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "                log_W1, log_W2 = None, None\n",
    "                if self.retention == 'kl':\n",
    "                    W1, W2 = F.softmax(W1, dim=-1), F.softmax(W2, dim=-1)\n",
    "                    log_W1, log_W2 = torch.log(W1.clamp(min=1e-10)), torch.log(W2.clamp(min=1e-10))\n",
    "\n",
    "                for t in range(T):\n",
    "                    k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "                    W1_leaf, W2_leaf = W1.detach().requires_grad_(True), W2.detach().requires_grad_(True)\n",
    "                    pred = self.memory_forward_deep(k_t.unsqueeze(1), W1_leaf, W2_leaf).squeeze(1)\n",
    "                    loss = self.get_loss(pred, v_t, x[:, t] if self.attentional_bias == 'huber' else None)\n",
    "                    grad1, grad2 = torch.autograd.grad(loss, [W1_leaf, W2_leaf])\n",
    "                    W1, log_W1 = self.apply_retention(W1, grad1, log_W1)\n",
    "                    W2, log_W2 = self.apply_retention(W2, grad2, log_W2)\n",
    "                    outputs.append(self.memory_forward_deep(q_t.unsqueeze(1), W1.detach(), W2.detach()).squeeze(1))\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "class MIRASBlock(nn.Module):\n",
    "    def __init__(self, d_model, memory_type, attentional_bias, retention, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.memory = MIRASLayer(d_model, memory_type, attentional_bias, retention)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_model * ffn_mult), nn.GELU(), nn.Linear(d_model * ffn_mult, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.memory(self.ln1(x))\n",
    "        return x + self.ffn(self.ln2(x))\n",
    "\n",
    "class MIRASLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, memory_type='deep', attentional_bias='l2', retention='l2', block_size=128):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "        self.layers = nn.ModuleList([MIRASBlock(d_model, memory_type, attentional_bias, retention) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.token_embedding.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None: torch.nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        x = self.token_embedding(idx) + self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        logits = self.lm_head(self.ln_f(x))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx[:, -self.block_size:])\n",
    "            probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "            idx = torch.cat((idx, torch.multinomial(probs, num_samples=1)), dim=1)\n",
    "        return idx\n",
    "\n",
    "# --- Build Model ---\n",
    "model = MIRASLanguageModel(\n",
    "    vocab_size=vocab_size, d_model=N_EMBD, n_layers=N_LAYERS,\n",
    "    memory_type=MEMORY_TYPE, attentional_bias=ATTENTIONAL_BIAS,\n",
    "    retention=RETENTION, block_size=BLOCK_SIZE\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN ===\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters=50):\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for iter in range(MAX_ITERS):\n",
    "    if iter % EVAL_INTERVAL == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "    _, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss()\n",
    "print(f\"Final: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a068b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERATE ===\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated = model.generate(context, max_new_tokens=MAX_NEW_TOKENS, temperature=TEMPERATURE)\n",
    "print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ea5f7",
   "metadata": {},
   "source": [
    "# Upload to HuggingFace\n",
    "\n",
    "Upload the trained MIRAS model to HuggingFace Hub. \n",
    "Make sure you have:\n",
    "1. A HuggingFace account\n",
    "2. An access token with write permissions (get one from https://huggingface.co/settings/tokens)\n",
    "3. `huggingface_hub` installed (`pip install huggingface_hub`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54222df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UPLOAD CONFIG ===\n",
    "\n",
    "HF_REPO_ID = \"av-codes/miras-shakespeare\"  # Change to your repo\n",
    "HF_TOKEN = '...'  # Set your token here or use huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a107b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UPLOAD TO HUGGINGFACE ===\n",
    "\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "import inspect\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "try:\n",
    "    create_repo(HF_REPO_ID, token=HF_TOKEN, exist_ok=True)\n",
    "    print(f\"Repository '{HF_REPO_ID}' ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    model_path = os.path.join(tmpdir, \"model.pt\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'vocab_size': vocab_size,\n",
    "        'd_model': N_EMBD,\n",
    "        'n_layers': N_LAYERS,\n",
    "        'block_size': BLOCK_SIZE,\n",
    "        'memory_type': MEMORY_TYPE,\n",
    "        'attentional_bias': ATTENTIONAL_BIAS,\n",
    "        'retention': RETENTION,\n",
    "    }, model_path)\n",
    "\n",
    "    config = {\n",
    "        'model_type': 'miras',\n",
    "        'vocab_size': vocab_size,\n",
    "        'd_model': N_EMBD,\n",
    "        'n_layers': N_LAYERS,\n",
    "        'block_size': BLOCK_SIZE,\n",
    "        'memory_type': MEMORY_TYPE,\n",
    "        'attentional_bias': ATTENTIONAL_BIAS,\n",
    "        'retention': RETENTION,\n",
    "        'chars': chars,\n",
    "    }\n",
    "    config_path = os.path.join(tmpdir, \"config.json\")\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "\n",
    "    modeling_code = '''\"\"\"MIRAS Language Model - Custom Architecture\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "def l2_loss(pred, target):\n",
    "    return 0.5 * ((pred - target) ** 2).sum(dim=-1)\n",
    "\n",
    "def lp_loss(pred, target, p=3):\n",
    "    return (torch.abs(pred - target) ** p).sum(dim=-1)\n",
    "\n",
    "def huber_loss(pred, target, delta):\n",
    "    diff = pred - target\n",
    "    abs_diff = torch.abs(diff)\n",
    "    return torch.where(abs_diff <= delta, 0.5 * diff ** 2, delta * (abs_diff - 0.5 * delta)).sum(dim=-1)\n",
    "\n",
    "def l2_retention_update(W, grad, alpha, eta):\n",
    "    return alpha * W - eta * grad\n",
    "\n",
    "def kl_retention_update(log_W, grad, alpha, eta, c=1.0):\n",
    "    log_W_new = alpha * log_W - eta * grad\n",
    "    return log_W_new, c * F.softmax(log_W_new, dim=-1)\n",
    "\n",
    "def elastic_net_update(W, grad, lambda_decay, zeta_lr, gamma_l1):\n",
    "    z = lambda_decay * W - zeta_lr * grad\n",
    "    return torch.sign(z) * F.relu(torch.abs(z) - gamma_l1)\n",
    "\n",
    "\n",
    "class KeyValueProjection(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.W_K(x), self.W_V(x), self.W_Q(x)\n",
    "\n",
    "\n",
    "class MIRASLayer(nn.Module):\n",
    "    def __init__(self, d, memory_type='deep', attentional_bias='l2', retention='l2', expansion=4, p=3, q=4):\n",
    "        super().__init__()\n",
    "        self.d, self.memory_type, self.attentional_bias, self.retention = d, memory_type, attentional_bias, retention\n",
    "        self.p, self.q = p, q\n",
    "        self.kv_proj = KeyValueProjection(d, d)\n",
    "\n",
    "        if memory_type == 'linear':\n",
    "            self.register_buffer('M_init', torch.zeros(d, d))\n",
    "        else:\n",
    "            self.W1_init = nn.Parameter(torch.randn(d, d * expansion) * 0.02)\n",
    "            self.W2_init = nn.Parameter(torch.randn(d * expansion, d) * 0.02)\n",
    "            self.ln = nn.LayerNorm(d)\n",
    "\n",
    "        if attentional_bias == 'huber':\n",
    "            self.delta_proj = nn.Linear(d, 1)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(1) * 0.9)\n",
    "        self.eta = nn.Parameter(torch.ones(1) * 0.1)\n",
    "        if retention == 'kl':\n",
    "            self.c = nn.Parameter(torch.ones(1))\n",
    "        if retention == 'elastic':\n",
    "            self.gamma = nn.Parameter(torch.ones(1) * 0.01)\n",
    "\n",
    "    def memory_forward_deep(self, x, W1, W2):\n",
    "        h = F.gelu(x @ W2.transpose(-2, -1))\n",
    "        return x + self.ln(h @ W1.transpose(-2, -1))\n",
    "\n",
    "    def get_loss(self, pred, target, x_t=None):\n",
    "        if self.attentional_bias == 'l2':\n",
    "            return l2_loss(pred, target).sum()\n",
    "        elif self.attentional_bias == 'lp':\n",
    "            return lp_loss(pred, target, self.p).sum()\n",
    "        else:\n",
    "            return huber_loss(pred, target, F.softplus(self.delta_proj(x_t))).sum()\n",
    "\n",
    "    def apply_retention(self, W, grad, log_W=None):\n",
    "        alpha, eta = torch.sigmoid(self.alpha), F.softplus(self.eta)\n",
    "        if self.retention == 'l2':\n",
    "            return l2_retention_update(W, grad, alpha, eta), None\n",
    "        elif self.retention == 'kl':\n",
    "            log_W = log_W if log_W is not None else torch.log(W.clamp(min=1e-10))\n",
    "            log_W_new, W_new = kl_retention_update(log_W, grad, alpha, eta, self.c)\n",
    "            return W_new, log_W_new\n",
    "        else:\n",
    "            return elastic_net_update(W, grad, alpha, eta, self.gamma), None\n",
    "\n",
    "    def forward(self, x):\n",
    "        k, v, q = self.kv_proj(x)\n",
    "        B, T, D = k.shape\n",
    "        outputs = []\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            if self.memory_type == 'linear':\n",
    "                M = self.M_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "                for t in range(T):\n",
    "                    k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "                    M_leaf = M.detach().requires_grad_(True)\n",
    "                    pred = torch.einsum('bde,be->bd', M_leaf, k_t)\n",
    "                    loss = self.get_loss(pred, v_t, x[:, t] if self.attentional_bias == 'huber' else None)\n",
    "                    grad = torch.autograd.grad(loss, M_leaf)[0]\n",
    "                    M, _ = self.apply_retention(M, grad)\n",
    "                    outputs.append(torch.einsum('bde,be->bd', M, q_t))\n",
    "            else:\n",
    "                W1 = self.W1_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "                W2 = self.W2_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "                log_W1, log_W2 = None, None\n",
    "                if self.retention == 'kl':\n",
    "                    W1, W2 = F.softmax(W1, dim=-1), F.softmax(W2, dim=-1)\n",
    "                    log_W1, log_W2 = torch.log(W1.clamp(min=1e-10)), torch.log(W2.clamp(min=1e-10))\n",
    "\n",
    "                for t in range(T):\n",
    "                    k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "                    W1_leaf, W2_leaf = W1.detach().requires_grad_(True), W2.detach().requires_grad_(True)\n",
    "                    pred = self.memory_forward_deep(k_t.unsqueeze(1), W1_leaf, W2_leaf).squeeze(1)\n",
    "                    loss = self.get_loss(pred, v_t, x[:, t] if self.attentional_bias == 'huber' else None)\n",
    "                    grad1, grad2 = torch.autograd.grad(loss, [W1_leaf, W2_leaf])\n",
    "                    W1, log_W1 = self.apply_retention(W1, grad1, log_W1)\n",
    "                    W2, log_W2 = self.apply_retention(W2, grad2, log_W2)\n",
    "                    outputs.append(self.memory_forward_deep(q_t.unsqueeze(1), W1.detach(), W2.detach()).squeeze(1))\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "\n",
    "class MIRASBlock(nn.Module):\n",
    "    def __init__(self, d_model, memory_type, attentional_bias, retention, ffn_mult=4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.memory = MIRASLayer(d_model, memory_type, attentional_bias, retention)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, d_model * ffn_mult), nn.GELU(), nn.Linear(d_model * ffn_mult, d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.memory(self.ln1(x))\n",
    "        return x + self.ffn(self.ln2(x))\n",
    "\n",
    "\n",
    "class MIRASLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, memory_type='deep', attentional_bias='l2', retention='l2', block_size=128):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "        self.layers = nn.ModuleList([MIRASBlock(d_model, memory_type, attentional_bias, retention) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.token_embedding.weight = self.lm_head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        x = self.token_embedding(idx) + self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        logits = self.lm_head(self.ln_f(x))\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) if targets is not None else None\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self(idx[:, -self.block_size:])\n",
    "            probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "            idx = torch.cat((idx, torch.multinomial(probs, num_samples=1)), dim=1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "def load_miras_model(repo_id_or_path, device='cpu'):\n",
    "    \"\"\"Load a MIRAS model from HuggingFace Hub or local path.\"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "\n",
    "    if Path(repo_id_or_path).exists():\n",
    "        base_path = Path(repo_id_or_path)\n",
    "        config_path = base_path / \"config.json\"\n",
    "        model_path = base_path / \"model.pt\"\n",
    "    else:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        config_path = hf_hub_download(repo_id=repo_id_or_path, filename=\"config.json\")\n",
    "        model_path = hf_hub_download(repo_id=repo_id_or_path, filename=\"model.pt\")\n",
    "\n",
    "    with open(config_path) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    model = MIRASLanguageModel(\n",
    "        vocab_size=config['vocab_size'],\n",
    "        d_model=config['d_model'],\n",
    "        n_layers=config['n_layers'],\n",
    "        memory_type=config['memory_type'],\n",
    "        attentional_bias=config['attentional_bias'],\n",
    "        retention=config['retention'],\n",
    "        block_size=config['block_size'],\n",
    "    )\n",
    "\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    stoi = {ch: i for i, ch in enumerate(config['chars'])}\n",
    "    itos = {i: ch for i, ch in enumerate(config['chars'])}\n",
    "    encode = lambda s: [stoi[c] for c in s]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "    return model, encode, decode, config\n",
    "'''\n",
    "\n",
    "    modeling_path = os.path.join(tmpdir, \"modeling_miras.py\")\n",
    "    with open(modeling_path, 'w') as f:\n",
    "        f.write(modeling_code)\n",
    "\n",
    "    readme_content = f\"\"\"# MIRAS Language Model\n",
    "\n",
    "A character-level language model trained on Shakespeare using the MIRAS (Memory-Integrated Recurrent Attention System) architecture.\n",
    "\n",
    "## Model Details\n",
    "- **Embedding dimension**: {N_EMBD}\n",
    "- **Layers**: {N_LAYERS}\n",
    "- **Block size**: {BLOCK_SIZE}\n",
    "- **Memory type**: {MEMORY_TYPE}\n",
    "- **Attentional bias**: {ATTENTIONAL_BIAS}\n",
    "- **Retention**: {RETENTION}\n",
    "- **Vocabulary size**: {vocab_size}\n",
    "\n",
    "## Installation\n",
    "\n",
    "```bash\n",
    "pip install torch huggingface_hub\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Quick Start\n",
    "\n",
    "```python\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "\n",
    "# Download files\n",
    "for f in [\"modeling_miras.py\", \"model.pt\", \"config.json\"]:\n",
    "    hf_hub_download(repo_id=\"{HF_REPO_ID}\", filename=f, local_dir=\"./miras\")\n",
    "\n",
    "# Import and load\n",
    "import sys\n",
    "sys.path.insert(0, \"./miras\")\n",
    "from modeling_miras import load_miras_model\n",
    "\n",
    "model, encode, decode, config = load_miras_model(\"./miras\")\n",
    "model.eval()\n",
    "\n",
    "# Generate text\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "output = model.generate(context, max_new_tokens=200, temperature=0.8)\n",
    "print(decode(output[0].tolist()))\n",
    "```\n",
    "\n",
    "### Using the Helper Function\n",
    "\n",
    "```python\n",
    "from modeling_miras import load_miras_model\n",
    "\n",
    "# Load directly from Hub\n",
    "model, encode, decode, config = load_miras_model(\"{HF_REPO_ID}\")\n",
    "\n",
    "# Generate\n",
    "import torch\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated = model.generate(context, max_new_tokens=100)\n",
    "print(decode(generated[0].tolist()))\n",
    "```\n",
    "\n",
    "## Files\n",
    "\n",
    "- `model.pt` - Model weights and architecture config\n",
    "- `config.json` - Full configuration including vocabulary\n",
    "- `modeling_miras.py` - Complete model architecture code\n",
    "\n",
    "## Training\n",
    "Trained for {MAX_ITERS} iterations on the TinyShakespeare dataset.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "MIRAS uses a novel memory-based attention mechanism with configurable:\n",
    "- **Memory type**: `linear` (matrix memory) or `deep` (MLP memory)\n",
    "- **Attentional bias**: `l2`, `lp`, or `huber` loss functions\n",
    "- **Retention**: `l2`, `kl`, or `elastic` weight update rules\n",
    "\"\"\"\n",
    "    readme_path = os.path.join(tmpdir, \"README.md\")\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "\n",
    "    api.upload_folder(\n",
    "        folder_path=tmpdir,\n",
    "        repo_id=HF_REPO_ID,\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "\n",
    "print(f\"âœ“ Model uploaded to https://huggingface.co/{HF_REPO_ID}\")\n",
    "print(f\"  Includes: model.pt, config.json, modeling_miras.py, README.md\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
