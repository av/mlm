{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730a53ac",
   "metadata": {},
   "source": [
    "# MIRAS: Memory, Attentional Bias, Retention & Algorithms\n",
    "\n",
    "> **Goal**: Implement a minimal, educational reproduction of the MIRAS framework from \"It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization\" (Behrouz et al. 2025).\n",
    "\n",
    "## Overview\n",
    "\n",
    "MIRAS (meaning \"Legacy\" in Persian/Arabic/Turkish) is a unified framework for designing sequence models based on four fundamental design choices:\n",
    "\n",
    "| Component | Description | Examples |\n",
    "|-----------|-------------|----------|\n",
    "| **Memory Architecture** | Vector, matrix, or deep MLP | Linear, 2-layer MLP |\n",
    "| **Attentional Bias** | Internal objective function defining \"similarity\" | ‚Ñì‚ÇÇ, ‚Ñì‚Çö, Huber, Dot-Product |\n",
    "| **Retention Gate** | How to balance learning new vs. retaining old | ‚Ñì‚ÇÇ, KL, Elastic Net |\n",
    "| **Memory Learning Algorithm** | The optimizer used | GD, GD+momentum |\n",
    "\n",
    "### Why MIRAS Matters\n",
    "\n",
    "1. **Unification**: Shows that Transformers, RNNs, and SSMs are all associative memory variants\n",
    "2. **Generalization**: Enables designing new architectures by mixing components\n",
    "3. **Robustness**: Novel attentional biases (Huber, ‚Ñì‚Çö) handle outliers better\n",
    "4. **Memory Management**: Novel retention gates provide better forgetting mechanisms\n",
    "\n",
    "### Models Covered\n",
    "\n",
    "| Model | Memory | Attentional Bias | Retention | Algorithm |\n",
    "|-------|--------|-----------------|-----------|-----------|\n",
    "| Linear Attention | Matrix | Dot-Product | - | GD |\n",
    "| DeltaNet | Matrix | ‚Ñì‚ÇÇ | - | GD |\n",
    "| Titans-LMM | k-layer MLP | ‚Ñì‚ÇÇ | ‚Ñì‚ÇÇ | GD+Momentum |\n",
    "| **Moneta** | 2-layer MLP | ‚Ñì‚Çö | ‚Ñìq | GD |\n",
    "| **Yaad** | 2-layer MLP | Huber | ‚Ñì‚ÇÇ | GD |\n",
    "| **Memora** | 2-layer MLP | ‚Ñì‚ÇÇ | KL | GD |\n",
    "\n",
    "Let's build this step by step! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9fcada",
   "metadata": {},
   "source": [
    "## Part 1: Theoretical Foundation\n",
    "\n",
    "### Core Concept: Associative Memory with Attentional Bias\n",
    "\n",
    "The fundamental insight of MIRAS is that most sequence models can be viewed as **associative memory modules** that learn a mapping from keys to values:\n",
    "\n",
    "$$M^* = \\arg\\min_M \\mathcal{L}(M(K); V)$$\n",
    "\n",
    "Where:\n",
    "- $K \\subseteq \\mathbb{R}^{d_k}$ are keys (projections of input)\n",
    "- $V \\subseteq \\mathbb{R}^{d_v}$ are values (projections of input)\n",
    "- $\\mathcal{L}$ is the **attentional bias** (internal objective function)\n",
    "- $M$ is the memory module (parameterized or non-parametric)\n",
    "\n",
    "### The Learning-Retaining Viewpoint\n",
    "\n",
    "$$W_t = \\arg\\min_W \\tilde{\\ell}_t(W; k_t, v_t) + \\text{Ret}_t(W, W_{t-1})$$\n",
    "\n",
    "- **First term**: Learn from new data\n",
    "- **Second term**: Retain previously learned knowledge\n",
    "\n",
    "This is a simple yet powerful formulation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30dcac1",
   "metadata": {},
   "source": [
    "## Part 2: Setup & Data\n",
    "\n",
    "Let's start with imports and loading our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32713b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "from typing import Optional, Tuple, Callable\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1fc3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1115394 bytes to dataset.txt\n",
      "Dataset length: 1115394 characters\n",
      "First 200 chars:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "def download_dataset(url: str, filename: str = \"dataset.txt\") -> str:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {len(response.content)} bytes to {filename}\")\n",
    "        return response.text\n",
    "    else:\n",
    "        raise RuntimeError(f\"Failed to download: {response.status_code}\")\n",
    "\n",
    "text = download_dataset(dataset_url)\n",
    "print(f\"Dataset length: {len(text)} characters\")\n",
    "print(f\"First 200 chars:\\n{text[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ebc531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n",
      "Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Example encoding: 'hello' -> [46, 43, 50, 50, 53]\n",
      "Example decoding: [46, 43, 50, 50, 53] -> 'hello'\n",
      "\n",
      "Data tensor shape: torch.Size([1115394])\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(f\"\\nExample encoding: 'hello' -> {encode('hello')}\")\n",
    "print(f\"Example decoding: {encode('hello')} -> '{decode(encode('hello'))}'\")\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(f\"\\nData tensor shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18162f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: 1003854 tokens\n",
      "Val data: 111540 tokens\n",
      "\n",
      "Batch shapes: x=torch.Size([32, 64]), y=torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32      # Reduced due to sequential memory processing\n",
    "block_size = 64      # Shorter context for memory efficiency\n",
    "n_embd = 128         # Larger embeddings (transformer uses 384)\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"Train data: {len(train_data)} tokens\")\n",
    "print(f\"Val data: {len(val_data)} tokens\")\n",
    "\n",
    "def get_batch(split: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    data_split = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(f\"\\nBatch shapes: x={xb.shape}, y={yb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b570c",
   "metadata": {},
   "source": [
    "## Part 3: Building Blocks\n",
    "\n",
    "### Key-Value Projections\n",
    "\n",
    "The first building block is projecting input embeddings into keys, values, and queries. This is shared across all memory architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c647bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 8, 128])\n",
      "K, V, Q shapes: torch.Size([2, 8, 128]), torch.Size([2, 8, 128]), torch.Size([2, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "class KeyValueProjection(nn.Module):\n",
    "    \"\"\"Projects input embeddings into keys, values, and queries.\n",
    "\n",
    "    This is the standard projection used in attention mechanisms.\n",
    "    In MIRAS, K and V are used for memory updates, Q is used for retrieval.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        super().__init__()\n",
    "        self.W_K = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_V = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_Q = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return self.W_K(x), self.W_V(x), self.W_Q(x)\n",
    "\n",
    "kv_proj = KeyValueProjection(n_embd, n_embd).to(device)\n",
    "test_x = torch.randn(2, 8, n_embd, device=device)\n",
    "k, v, q = kv_proj(test_x)\n",
    "print(f\"Input shape: {test_x.shape}\")\n",
    "print(f\"K, V, Q shapes: {k.shape}, {v.shape}, {q.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f93a576",
   "metadata": {},
   "source": [
    "### Linear Memory: Hebbian Rule (Linear Attention)\n",
    "\n",
    "The simplest associative memory uses the **Hebbian update**:\n",
    "\n",
    "$$M_t = \\alpha M_{t-1} + v_t k_t^\\top$$\n",
    "\n",
    "This is equivalent to **Linear Attention** ‚Äî it simply accumulates key-value associations.\n",
    "\n",
    "- **Memory**: $M \\in \\mathbb{R}^{d \\times d}$ (matrix)\n",
    "- **Attentional Bias**: Dot-product similarity\n",
    "- **Retention**: Exponential decay ($\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "476fb861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hebbian memory output shape: torch.Size([2, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "class LinearMemoryHebbian(nn.Module):\n",
    "    \"\"\"Simplest associative memory - equivalent to Linear Attention.\n",
    "\n",
    "    Update rule: M_t = Œ± * M_{t-1} + v_t ‚äó k_t\n",
    "    Retrieval:   y_t = M_t @ q_t\n",
    "\n",
    "    This is a \"write everything, forget slowly\" approach.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "\n",
    "    def forward(self, keys: torch.Tensor, values: torch.Tensor,\n",
    "                queries: torch.Tensor, alpha: float = 0.9) -> torch.Tensor:\n",
    "        B, T, D = keys.shape\n",
    "        M = torch.zeros(B, D, D, device=keys.device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(T):\n",
    "            k_t = keys[:, t]      # [B, D]\n",
    "            v_t = values[:, t]    # [B, D]\n",
    "            q_t = queries[:, t]   # [B, D]\n",
    "\n",
    "            M = alpha * M + torch.einsum('bd,be->bde', v_t, k_t)\n",
    "            y_t = torch.einsum('bde,be->bd', M, q_t)\n",
    "            outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "mem_hebbian = LinearMemoryHebbian(n_embd).to(device)\n",
    "out = mem_hebbian(k, v, q)\n",
    "print(f\"Hebbian memory output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a700bbed",
   "metadata": {},
   "source": [
    "### Linear Memory: Delta Rule (DeltaNet)\n",
    "\n",
    "The **Delta Rule** is smarter: it removes the old value before writing the new one:\n",
    "\n",
    "$$M_t = \\alpha(I - \\eta k_t k_t^\\top) M_{t-1} + v_t k_t^\\top$$\n",
    "\n",
    "The $(I - \\eta k_t k_t^\\top)$ term \"erases\" the old association for $k_t$ before writing the new one.\n",
    "\n",
    "**Key insight**: The learning rate $\\eta$ only appears in the \"erase\" term, NOT in the \"write\" term!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c13e627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta memory output shape: torch.Size([2, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "class LinearMemoryDelta(nn.Module):\n",
    "    \"\"\"Delta rule memory - removes old value before writing new.\n",
    "\n",
    "    Update rule: M_t = Œ± * (I - Œ∑ * k_t ‚äó k_t) @ M_{t-1} + v_t ‚äó k_t\n",
    "\n",
    "    The (I - Œ∑ * k_t ‚äó k_t) term \"erases\" the old value associated with k_t\n",
    "    before writing the new association v_t ‚äó k_t.\n",
    "\n",
    "    This is more like \"overwrite\" than \"accumulate\".\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "\n",
    "    def forward(self, keys: torch.Tensor, values: torch.Tensor,\n",
    "                queries: torch.Tensor, alpha: float = 0.9, eta: float = 0.1) -> torch.Tensor:\n",
    "        B, T, D = keys.shape\n",
    "        M = torch.zeros(B, D, D, device=keys.device)\n",
    "        I = torch.eye(D, device=keys.device).unsqueeze(0)  # [1, D, D]\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(T):\n",
    "            k_t = keys[:, t]      # [B, D]\n",
    "            v_t = values[:, t]    # [B, D]\n",
    "            q_t = queries[:, t]   # [B, D]\n",
    "\n",
    "            kk = torch.einsum('bd,be->bde', k_t, k_t)  # [B, D, D]\n",
    "            M = alpha * torch.bmm(I - eta * kk, M)     # Erase old\n",
    "            M = M + torch.einsum('bd,be->bde', v_t, k_t)  # Write new (no eta!)\n",
    "\n",
    "            y_t = torch.einsum('bde,be->bd', M, q_t)\n",
    "            outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "mem_delta = LinearMemoryDelta(n_embd).to(device)\n",
    "out = mem_delta(k, v, q)\n",
    "print(f\"Delta memory output shape: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86e64da",
   "metadata": {},
   "source": [
    "### Deep Memory Module (Titans/MIRAS)\n",
    "\n",
    "Linear memory assumes linear dependencies. **Deep memory** can learn non-linear patterns.\n",
    "\n",
    "The architecture (post-norm per MIRAS Eq. 5):\n",
    "$$M(x) = x + \\text{LayerNorm}(W_1 \\sigma(W_2 x))$$\n",
    "\n",
    "Where:\n",
    "- $W_2 \\in \\mathbb{R}^{h \\times d}$ projects up (expansion)\n",
    "- $W_1 \\in \\mathbb{R}^{d \\times h}$ projects down\n",
    "- $\\sigma$ is GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68171732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep memory: torch.Size([2, 8, 128]) -> torch.Size([2, 8, 128])\n",
      "Parameters: 131,328\n"
     ]
    }
   ],
   "source": [
    "class DeepMemory(nn.Module):\n",
    "    \"\"\"2-layer MLP memory as in Titans/MIRAS (post-norm architecture).\n",
    "\n",
    "    M(x) = x + LayerNorm(W1 @ œÉ(W2 @ x))\n",
    "\n",
    "    This can learn non-linear key-value associations.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, expansion: int = 4):\n",
    "        super().__init__()\n",
    "        self.W2 = nn.Linear(d, d * expansion, bias=False)  # Up projection\n",
    "        self.W1 = nn.Linear(d * expansion, d, bias=False)  # Down projection\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.gelu(self.W2(x))\n",
    "        return x + self.ln(self.W1(h))\n",
    "\n",
    "deep_mem = DeepMemory(n_embd).to(device)\n",
    "test_input = torch.randn(2, 8, n_embd, device=device)\n",
    "test_output = deep_mem(test_input)\n",
    "print(f\"Deep memory: {test_input.shape} -> {test_output.shape}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in deep_mem.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11ce2c",
   "metadata": {},
   "source": [
    "## Part 4: Attentional Bias Objectives\n",
    "\n",
    "The **attentional bias** is the internal objective function that defines what \"similarity\" means for the memory. Different choices lead to different behaviors:\n",
    "\n",
    "| Objective | Formula | Properties |\n",
    "|-----------|---------|------------|\n",
    "| ‚Ñì‚ÇÇ (MSE) | $\\frac{1}{2}\\|M(k) - v\\|_2^2$ | Standard, smooth |\n",
    "| ‚Ñì‚Çö | $\\|M(k) - v\\|_p^p$ | Robust for $p < 2$ |\n",
    "| Huber | Quadratic near 0, linear far | Robust to outliers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35d2267c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Ñì‚ÇÇ loss: tensor([127.6315, 132.5224, 120.8685, 147.0225], device='cuda:0')\n",
      "‚Ñì‚ÇÉ loss: tensor([641.6620, 566.5828, 533.0881, 735.3480], device='cuda:0')\n",
      "Huber loss: tensor([55.2330, 61.3017, 55.1498, 61.9771], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def l2_loss(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Standard ‚Ñì‚ÇÇ loss (MSE without mean reduction).\n",
    "\n",
    "    Returns per-sample loss for gradient computation.\n",
    "    \"\"\"\n",
    "    return 0.5 * ((pred - target) ** 2).sum(dim=-1)\n",
    "\n",
    "\n",
    "def lp_loss(pred: torch.Tensor, target: torch.Tensor, p: float = 3) -> torch.Tensor:\n",
    "    \"\"\"‚Ñì‚Çö loss - more robust for p < 2, more sensitive for p > 2.\n",
    "\n",
    "    p=1: Manhattan distance (most robust to outliers)\n",
    "    p=2: Euclidean distance (standard)\n",
    "    p=3+: More sensitive to large errors\n",
    "    \"\"\"\n",
    "    return (torch.abs(pred - target) ** p).sum(dim=-1)\n",
    "\n",
    "\n",
    "def huber_loss(pred: torch.Tensor, target: torch.Tensor, delta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Huber loss - robust to outliers.\n",
    "\n",
    "    Behaves like ‚Ñì‚ÇÇ for small errors, ‚Ñì‚ÇÅ for large errors.\n",
    "    Delta controls the transition point.\n",
    "    \"\"\"\n",
    "    diff = pred - target\n",
    "    abs_diff = torch.abs(diff)\n",
    "    quadratic = 0.5 * diff ** 2\n",
    "    linear = delta * (abs_diff - 0.5 * delta)\n",
    "    return torch.where(abs_diff <= delta, quadratic, linear).sum(dim=-1)\n",
    "\n",
    "pred = torch.randn(4, n_embd, device=device)\n",
    "target = torch.randn(4, n_embd, device=device)\n",
    "delta = torch.ones(4, 1, device=device) * 0.5\n",
    "\n",
    "print(f\"‚Ñì‚ÇÇ loss: {l2_loss(pred, target)}\")\n",
    "print(f\"‚Ñì‚ÇÉ loss: {lp_loss(pred, target, p=3)}\")\n",
    "print(f\"Huber loss: {huber_loss(pred, target, delta)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e56ecd2",
   "metadata": {},
   "source": [
    "### Gradient Computation for Memory Updates\n",
    "\n",
    "For deep memory, we use PyTorch autograd to compute gradients. For linear memory, we can also compute closed-form gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3adfcc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Ñì‚ÇÇ gradient shape: torch.Size([2, 128, 128])\n",
      "‚Ñì‚ÇÉ gradient shape: torch.Size([2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "def compute_memory_grad(memory: nn.Module, k: torch.Tensor, v: torch.Tensor,\n",
    "                        loss_fn: Callable) -> Tuple[torch.Tensor, ...]:\n",
    "    \"\"\"Compute gradient of loss w.r.t. memory parameters using autograd.\n",
    "\n",
    "    Works for any memory architecture (linear or deep).\n",
    "    \"\"\"\n",
    "    for p in memory.parameters():\n",
    "        if p.grad is not None:\n",
    "            p.grad.zero_()\n",
    "\n",
    "    pred = memory(k)\n",
    "    loss = loss_fn(pred, v).sum()\n",
    "    grads = torch.autograd.grad(loss, memory.parameters(), create_graph=False)\n",
    "    return grads\n",
    "\n",
    "\n",
    "def apply_memory_update(memory: nn.Module, grads: Tuple[torch.Tensor, ...],\n",
    "                        alpha: float, eta: float) -> None:\n",
    "    \"\"\"Apply gradient update with retention (weight decay).\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param, grad in zip(memory.parameters(), grads):\n",
    "            param.mul_(alpha).sub_(eta * grad)\n",
    "\n",
    "\n",
    "def l2_gradient_linear(M: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Closed-form gradient of ‚Ñì‚ÇÇ loss for linear memory: (Mk - v) @ k.T\"\"\"\n",
    "    pred = torch.einsum('bde,be->bd', M, k)  # M @ k\n",
    "    error = pred - v\n",
    "    return torch.einsum('bd,be->bde', error, k)  # error @ k.T\n",
    "\n",
    "\n",
    "def lp_gradient_linear(M: torch.Tensor, k: torch.Tensor, v: torch.Tensor,\n",
    "                       p: float = 3, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"Closed-form gradient of ‚Ñì‚Çö loss for linear memory with smooth approximations.\"\"\"\n",
    "    pred = torch.einsum('bde,be->bd', M, k)\n",
    "    diff = pred - v\n",
    "    sign_diff = torch.tanh(100 * diff)  # Smooth sign approximation\n",
    "    abs_diff = torch.sqrt(diff ** 2 + eps)  # Smooth abs\n",
    "    grad_coef = p * sign_diff * (abs_diff ** (p - 1))\n",
    "    return torch.einsum('bd,be->bde', grad_coef, k)\n",
    "\n",
    "M = torch.randn(2, n_embd, n_embd, device=device)\n",
    "k_test = torch.randn(2, n_embd, device=device)\n",
    "v_test = torch.randn(2, n_embd, device=device)\n",
    "\n",
    "grad_l2 = l2_gradient_linear(M, k_test, v_test)\n",
    "grad_l3 = lp_gradient_linear(M, k_test, v_test, p=3)\n",
    "print(f\"‚Ñì‚ÇÇ gradient shape: {grad_l2.shape}\")\n",
    "print(f\"‚Ñì‚ÇÉ gradient shape: {grad_l3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd9247b",
   "metadata": {},
   "source": [
    "## Part 5: Retention Gates\n",
    "\n",
    "**Retention** controls how much of the old memory to keep when learning new information.\n",
    "\n",
    "| Retention | Update Form | Properties |\n",
    "|-----------|-------------|------------|\n",
    "| ‚Ñì‚ÇÇ | $W_t = \\alpha W_{t-1} - \\eta \\nabla$ | Simple weight decay |\n",
    "| KL | $W_t = c \\cdot \\text{Softmax}(\\alpha \\log W_{t-1} - \\eta \\nabla)$ | Entropy regularized |\n",
    "| Elastic Net | $W_t = \\text{soft\\_threshold}(\\lambda W_{t-1} - \\zeta \\nabla, \\gamma)$ | Sparse forgetting |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ff1ef9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Ñì‚ÇÇ retention update: torch.Size([4, 8])\n",
      "KL retention update: torch.Size([4, 8])\n",
      "Elastic net update: torch.Size([4, 8])\n",
      "Sparsity (elastic net zeros): 0/32\n"
     ]
    }
   ],
   "source": [
    "def l2_retention_update(W: torch.Tensor, grad: torch.Tensor,\n",
    "                        alpha: float, eta: float) -> torch.Tensor:\n",
    "    \"\"\"Standard ‚Ñì‚ÇÇ retention (weight decay) update.\n",
    "\n",
    "    W_t = Œ± * W_{t-1} - Œ∑ * ‚àá‚Ñì\n",
    "\n",
    "    The ‚Ñì‚ÇÇ regularization induces exponential decay of old weights.\n",
    "    \"\"\"\n",
    "    return alpha * W - eta * grad\n",
    "\n",
    "\n",
    "def kl_retention_update(log_W: torch.Tensor, grad: torch.Tensor,\n",
    "                        alpha: float, eta: float, c: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"KL divergence retention (Memora) per MIRAS Equation 27.\n",
    "\n",
    "    Works in log domain: log_W_t = Œ± * log_W_{t-1} - Œ∑ * ‚àá‚Ñì\n",
    "    Then: W_t = c * Softmax(log_W_t)\n",
    "\n",
    "    Returns both the updated log_W and the actual W.\n",
    "    \"\"\"\n",
    "    log_W_new = alpha * log_W - eta * grad\n",
    "    W_new = c * F.softmax(log_W_new, dim=-1)\n",
    "    return log_W_new, W_new\n",
    "\n",
    "\n",
    "def soft_threshold(z: torch.Tensor, gamma: float) -> torch.Tensor:\n",
    "    \"\"\"Proximal operator for ‚Ñì‚ÇÅ regularization (soft thresholding).\n",
    "\n",
    "    This is the key operation for elastic net / sparse forgetting.\n",
    "    \"\"\"\n",
    "    return torch.sign(z) * F.relu(torch.abs(z) - gamma)\n",
    "\n",
    "\n",
    "def elastic_net_update(W: torch.Tensor, grad: torch.Tensor,\n",
    "                       lambda_decay: float, zeta_lr: float, gamma_l1: float) -> torch.Tensor:\n",
    "    \"\"\"Elastic net update combining ‚Ñì‚ÇÇ decay with ‚Ñì‚ÇÅ sparsity.\n",
    "\n",
    "    W_t = soft_threshold(Œª * W_{t-1} - Œ∂ * ‚àá‚Ñì, Œ≥)\n",
    "\n",
    "    Combines feature selection (‚Ñì‚ÇÅ) with bias reduction (‚Ñì‚ÇÇ).\n",
    "    \"\"\"\n",
    "    return soft_threshold(lambda_decay * W - zeta_lr * grad, gamma_l1)\n",
    "\n",
    "W_test = torch.randn(4, 8, device=device)\n",
    "grad_test = torch.randn(4, 8, device=device)\n",
    "\n",
    "W_l2 = l2_retention_update(W_test, grad_test, 0.9, 0.1)\n",
    "_, W_kl = kl_retention_update(W_test, grad_test, 0.9, 0.1, c=1.0)\n",
    "W_elastic = elastic_net_update(W_test, grad_test, 0.9, 0.1, 0.01)\n",
    "\n",
    "print(f\"‚Ñì‚ÇÇ retention update: {W_l2.shape}\")\n",
    "print(f\"KL retention update: {W_kl.shape}\")\n",
    "print(f\"Elastic net update: {W_elastic.shape}\")\n",
    "print(f\"Sparsity (elastic net zeros): {(W_elastic.abs() < 1e-6).sum().item()}/{W_elastic.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e436e44",
   "metadata": {},
   "source": [
    "## Part 6: Three Novel MIRAS Models\n",
    "\n",
    "Now we combine the building blocks into complete models. Each model has:\n",
    "- A **deep memory** (2-layer MLP)\n",
    "- A specific **attentional bias** (objective function)\n",
    "- A specific **retention gate** (forgetting mechanism)\n",
    "\n",
    "### Model 1: Moneta (‚Ñì‚Çö-‚Ñìq-Moneta)\n",
    "\n",
    "**Configuration**:\n",
    "- Memory: 2-layer MLP with GELU, residual + LayerNorm\n",
    "- Attentional Bias: ‚Ñì‚Çö (p=3)\n",
    "- Retention: ‚Ñìq normalization (q=4)\n",
    "- Algorithm: Gradient Descent\n",
    "\n",
    "The ‚Ñìq normalization ensures memory weights don't explode:\n",
    "$$W_t = \\frac{A_t}{\\|A_t\\|^{(q-2)/q}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edeeb088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moneta output shape: torch.Size([2, 8, 128])\n",
      "Moneta parameters: 180,480\n"
     ]
    }
   ],
   "source": [
    "class Moneta(nn.Module):\n",
    "    \"\"\"‚Ñì‚Çö attentional bias + ‚Ñìq retention.\n",
    "\n",
    "    Uses a functional approach: maintains memory state explicitly\n",
    "    rather than modifying nn.Module parameters in-place.\n",
    "\n",
    "    The ‚Ñìq normalization keeps weights bounded while allowing\n",
    "    ‚Ñì‚Çö gradients to shape the learning dynamics.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, expansion: int = 4, p: float = 3, q: float = 4):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.expansion = expansion\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.kv_proj = KeyValueProjection(d, d)\n",
    "\n",
    "        self.W1_init = nn.Parameter(torch.randn(d, d * expansion) * 0.02)\n",
    "        self.W2_init = nn.Parameter(torch.randn(d * expansion, d) * 0.02)\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "\n",
    "    def memory_forward(self, x: torch.Tensor, W1: torch.Tensor, W2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Functional memory forward pass.\"\"\"\n",
    "        h = F.gelu(x @ W2.transpose(-2, -1))\n",
    "        return x + self.ln(h @ W1.transpose(-2, -1))\n",
    "\n",
    "    def lq_normalize(self, A: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"‚Ñìq normalization: W = A / ||A||^((q-2)/q)\"\"\"\n",
    "        norm = torch.norm(A, dim=(-2, -1), keepdim=True).clamp(min=1e-8)\n",
    "        return A / (norm ** ((self.q - 2) / self.q))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, alpha: float = 0.9, eta: float = 0.1) -> torch.Tensor:\n",
    "        k, v, q = self.kv_proj(x)\n",
    "        B, T, D = k.shape\n",
    "\n",
    "        A1 = self.W1_init.clone().unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "        A2 = self.W2_init.clone().unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "        outputs = []\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            for t in range(T):\n",
    "                k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "\n",
    "                W1 = self.lq_normalize(A1)\n",
    "                W2 = self.lq_normalize(A2)\n",
    "\n",
    "                W1_leaf = W1.detach().requires_grad_(True)\n",
    "                W2_leaf = W2.detach().requires_grad_(True)\n",
    "\n",
    "                pred = self.memory_forward(k_t.unsqueeze(1), W1_leaf, W2_leaf).squeeze(1)\n",
    "                loss = lp_loss(pred, v_t, self.p).sum()\n",
    "                grad1, grad2 = torch.autograd.grad(loss, [W1_leaf, W2_leaf])\n",
    "\n",
    "                A1 = alpha * A1 - eta * grad1\n",
    "                A2 = alpha * A2 - eta * grad2\n",
    "\n",
    "                y_t = self.memory_forward(q_t.unsqueeze(1), W1, W2).squeeze(1)\n",
    "                outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "moneta = Moneta(n_embd).to(device)\n",
    "test_emb = torch.randn(2, 8, n_embd, device=device)\n",
    "out = moneta(test_emb)\n",
    "print(f\"Moneta output shape: {out.shape}\")\n",
    "print(f\"Moneta parameters: {sum(p.numel() for p in moneta.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbcc7bd",
   "metadata": {},
   "source": [
    "### Model 2: Yaad (Robust Memory with Coping)\n",
    "\n",
    "**Yaad** (memory in Persian) uses **Huber loss** as the attentional bias, making it robust to outliers.\n",
    "\n",
    "**Configuration**:\n",
    "- Memory: 2-layer MLP with GELU, residual + LayerNorm\n",
    "- Attentional Bias: Huber loss (data-dependent threshold Œ¥)\n",
    "- Retention: ‚Ñì‚ÇÇ (standard weight decay)\n",
    "- Algorithm: Gradient Descent\n",
    "\n",
    "The insight: like human memory's coping mechanisms, extreme events (outliers) are processed differently than normal events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c34c4e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yaad output shape: torch.Size([2, 8, 128])\n",
      "Yaad parameters: 180,609\n"
     ]
    }
   ],
   "source": [
    "class Yaad(nn.Module):\n",
    "    \"\"\"Huber attentional bias - robust to outliers.\n",
    "\n",
    "    Switches between ‚Ñì‚ÇÇ and ‚Ñì‚ÇÅ gradients based on error magnitude,\n",
    "    providing robustness similar to human memory's coping mechanisms.\n",
    "\n",
    "    For small errors: behave like ‚Ñì‚ÇÇ (smooth, precise)\n",
    "    For large errors: behave like ‚Ñì‚ÇÅ (robust, less sensitive)\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, expansion: int = 4):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.kv_proj = KeyValueProjection(d, d)\n",
    "        self.delta_proj = nn.Linear(d, 1)  # Data-dependent threshold\n",
    "\n",
    "        self.W1_init = nn.Parameter(torch.randn(d, d * expansion) * 0.02)\n",
    "        self.W2_init = nn.Parameter(torch.randn(d * expansion, d) * 0.02)\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "\n",
    "    def memory_forward(self, x: torch.Tensor, W1: torch.Tensor, W2: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.gelu(x @ W2.transpose(-2, -1))\n",
    "        return x + self.ln(h @ W1.transpose(-2, -1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, alpha: float = 0.9, eta: float = 0.1) -> torch.Tensor:\n",
    "        k, v, q = self.kv_proj(x)\n",
    "        B, T, D = k.shape\n",
    "\n",
    "        W1 = self.W1_init.clone().unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "        W2 = self.W2_init.clone().unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "        outputs = []\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            for t in range(T):\n",
    "                k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "\n",
    "                W1_leaf = W1.detach().requires_grad_(True)\n",
    "                W2_leaf = W2.detach().requires_grad_(True)\n",
    "\n",
    "                pred = self.memory_forward(k_t.unsqueeze(1), W1_leaf, W2_leaf).squeeze(1)\n",
    "                delta_t = F.softplus(self.delta_proj(x[:, t]))  # [B, 1]\n",
    "\n",
    "                loss = huber_loss(pred, v_t, delta_t).sum()\n",
    "                grad1, grad2 = torch.autograd.grad(loss, [W1_leaf, W2_leaf])\n",
    "\n",
    "                W1 = alpha * W1 - eta * grad1\n",
    "                W2 = alpha * W2 - eta * grad2\n",
    "\n",
    "                y_t = self.memory_forward(q_t.unsqueeze(1), W1.detach(), W2.detach()).squeeze(1)\n",
    "                outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "yaad = Yaad(n_embd).to(device)\n",
    "out = yaad(test_emb)\n",
    "print(f\"Yaad output shape: {out.shape}\")\n",
    "print(f\"Yaad parameters: {sum(p.numel() for p in yaad.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfe52d1",
   "metadata": {},
   "source": [
    "### Model 3: Memora (Entropy-Regularized Memory)\n",
    "\n",
    "**Memora** uses **KL divergence** retention, keeping memory weights on a scaled probability simplex.\n",
    "\n",
    "**Configuration**:\n",
    "- Memory: 2-layer MLP with GELU, residual + LayerNorm\n",
    "- Attentional Bias: ‚Ñì‚ÇÇ\n",
    "- Retention: KL divergence with scaling constant $c$\n",
    "- Algorithm: Gradient Descent\n",
    "\n",
    "The update rule (MIRAS Eq. 27):\n",
    "$$W_t = c \\cdot \\text{Softmax}(\\alpha_t \\log(W_{t-1}) - \\eta_t \\nabla\\ell_2)$$\n",
    "\n",
    "The softmax ensures numerical stability and the scaling constant $c$ controls output magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78146e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memora output shape: torch.Size([2, 8, 128])\n",
      "Memora parameters: 49,409\n"
     ]
    }
   ],
   "source": [
    "class Memora(nn.Module):\n",
    "    \"\"\"KL divergence retention - entropy-regularized memory (MIRAS Eq. 27).\n",
    "\n",
    "    W_t = c * Softmax(Œ±_t * log(W_{t-1}) - Œ∑_t * ‚àá‚Ñì‚ÇÇ)\n",
    "\n",
    "    Works in log-domain for numerical stability. The softmax ensures\n",
    "    weights stay on a scaled probability simplex.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, expansion: int = 4, c: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.kv_proj = KeyValueProjection(d, d)\n",
    "        self.c = nn.Parameter(torch.tensor(c))\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "\n",
    "        W1_raw = torch.rand(d, d * expansion) + 0.1\n",
    "        W2_raw = torch.rand(d * expansion, d) + 0.1\n",
    "        self.register_buffer('W1_init', F.softmax(W1_raw, dim=-1))\n",
    "        self.register_buffer('W2_init', F.softmax(W2_raw, dim=-1))\n",
    "\n",
    "    def memory_forward(self, x: torch.Tensor, W1: torch.Tensor, W2: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.gelu(x @ W2.transpose(-2, -1))\n",
    "        return x + self.ln(h @ W1.transpose(-2, -1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, alpha: float = 0.9, eta: float = 0.1) -> torch.Tensor:\n",
    "        k, v, q = self.kv_proj(x)\n",
    "        B, T, D = k.shape\n",
    "\n",
    "        log_W1 = torch.log(self.W1_init.clamp(min=1e-10)).unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "        log_W2 = torch.log(self.W2_init.clamp(min=1e-10)).unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "        outputs = []\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            for t in range(T):\n",
    "                k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "\n",
    "                W1 = self.c * F.softmax(log_W1, dim=-1)\n",
    "                W2 = self.c * F.softmax(log_W2, dim=-1)\n",
    "\n",
    "                W1_leaf = W1.detach().requires_grad_(True)\n",
    "                W2_leaf = W2.detach().requires_grad_(True)\n",
    "\n",
    "                pred = self.memory_forward(k_t.unsqueeze(1), W1_leaf, W2_leaf).squeeze(1)\n",
    "                loss = l2_loss(pred, v_t).sum()\n",
    "                grad1, grad2 = torch.autograd.grad(loss, [W1_leaf, W2_leaf])\n",
    "\n",
    "                log_W1 = alpha * log_W1 - eta * grad1\n",
    "                log_W2 = alpha * log_W2 - eta * grad2\n",
    "\n",
    "                W1_out = self.c * F.softmax(log_W1.detach(), dim=-1)\n",
    "                W2_out = self.c * F.softmax(log_W2.detach(), dim=-1)\n",
    "                y_t = self.memory_forward(q_t.unsqueeze(1), W1_out, W2_out).squeeze(1)\n",
    "                outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "memora = Memora(n_embd).to(device)\n",
    "out = memora(test_emb)\n",
    "print(f\"Memora output shape: {out.shape}\")\n",
    "print(f\"Memora parameters: {sum(p.numel() for p in memora.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e961c",
   "metadata": {},
   "source": [
    "## Part 7: Unified MIRAS Layer\n",
    "\n",
    "Now let's create a configurable MIRAS layer that can use any combination of:\n",
    "- Memory architecture (linear or deep)\n",
    "- Attentional bias (‚Ñì‚ÇÇ, ‚Ñì‚Çö, Huber)\n",
    "- Retention gate (‚Ñì‚ÇÇ, KL, elastic net)\n",
    "\n",
    "This is the power of the MIRAS framework ‚Äî mix and match components!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b775fc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIRAS ‚Ñì‚ÇÇ: torch.Size([2, 8, 128])\n",
      "MIRAS ‚Ñì‚Çö: torch.Size([2, 8, 128])\n",
      "MIRAS Huber: torch.Size([2, 8, 128])\n"
     ]
    }
   ],
   "source": [
    "class MIRASLayer(nn.Module):\n",
    "    \"\"\"Complete MIRAS layer with configurable components.\n",
    "\n",
    "    This is the unified layer that can instantiate any model from the MIRAS family\n",
    "    by choosing the appropriate memory, attentional bias, and retention.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int,\n",
    "                 memory_type: str = 'deep',      # 'linear', 'deep'\n",
    "                 attentional_bias: str = 'l2',   # 'l2', 'lp', 'huber'\n",
    "                 retention: str = 'l2',          # 'l2', 'kl', 'elastic'\n",
    "                 expansion: int = 4,\n",
    "                 p: float = 3,\n",
    "                 q: float = 4):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.memory_type = memory_type\n",
    "        self.attentional_bias = attentional_bias\n",
    "        self.retention = retention\n",
    "        self.p, self.q = p, q\n",
    "\n",
    "        self.kv_proj = KeyValueProjection(d, d)\n",
    "\n",
    "        if memory_type == 'linear':\n",
    "            self.register_buffer('M_init', torch.zeros(d, d))\n",
    "        else:\n",
    "            self.W1_init = nn.Parameter(torch.randn(d, d * expansion) * 0.02)\n",
    "            self.W2_init = nn.Parameter(torch.randn(d * expansion, d) * 0.02)\n",
    "            self.ln = nn.LayerNorm(d)\n",
    "\n",
    "        if attentional_bias == 'huber':\n",
    "            self.delta_proj = nn.Linear(d, 1)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(1) * 0.9)\n",
    "        self.eta = nn.Parameter(torch.ones(1) * 0.1)\n",
    "\n",
    "        if retention == 'kl':\n",
    "            self.c = nn.Parameter(torch.ones(1))\n",
    "        if retention == 'elastic':\n",
    "            self.gamma = nn.Parameter(torch.ones(1) * 0.01)\n",
    "\n",
    "    def memory_forward_deep(self, x: torch.Tensor, W1: torch.Tensor, W2: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.gelu(x @ W2.transpose(-2, -1))\n",
    "        return x + self.ln(h @ W1.transpose(-2, -1))\n",
    "\n",
    "    def get_loss(self, pred: torch.Tensor, target: torch.Tensor, x_t: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if self.attentional_bias == 'l2':\n",
    "            return l2_loss(pred, target).sum()\n",
    "        elif self.attentional_bias == 'lp':\n",
    "            return lp_loss(pred, target, self.p).sum()\n",
    "        elif self.attentional_bias == 'huber':\n",
    "            delta = F.softplus(self.delta_proj(x_t))\n",
    "            return huber_loss(pred, target, delta).sum()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown attentional bias: {self.attentional_bias}\")\n",
    "\n",
    "    def apply_retention(self, W: torch.Tensor, grad: torch.Tensor, log_W: Optional[torch.Tensor] = None):\n",
    "        alpha = torch.sigmoid(self.alpha)\n",
    "        eta = F.softplus(self.eta)\n",
    "\n",
    "        if self.retention == 'l2':\n",
    "            return l2_retention_update(W, grad, alpha, eta), None\n",
    "        elif self.retention == 'kl':\n",
    "            if log_W is None:\n",
    "                log_W = torch.log(W.clamp(min=1e-10))\n",
    "            log_W_new, W_new = kl_retention_update(log_W, grad, alpha, eta, self.c)\n",
    "            return W_new, log_W_new\n",
    "        elif self.retention == 'elastic':\n",
    "            return elastic_net_update(W, grad, alpha, eta, self.gamma), None\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown retention: {self.retention}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        k, v, q = self.kv_proj(x)\n",
    "        B, T, D = k.shape\n",
    "        outputs = []\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            if self.memory_type == 'linear':\n",
    "                M = self.M_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "\n",
    "                for t in range(T):\n",
    "                    k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "                    M_leaf = M.detach().requires_grad_(True)\n",
    "                    pred = torch.einsum('bde,be->bd', M_leaf, k_t)\n",
    "                    loss = self.get_loss(pred, v_t, x[:, t] if self.attentional_bias == 'huber' else None)\n",
    "                    grad = torch.autograd.grad(loss, M_leaf)[0]\n",
    "                    M, _ = self.apply_retention(M, grad)\n",
    "                    y_t = torch.einsum('bde,be->bd', M, q_t)\n",
    "                    outputs.append(y_t)\n",
    "            else:\n",
    "                W1 = self.W1_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "                W2 = self.W2_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "                log_W1, log_W2 = None, None\n",
    "\n",
    "                if self.retention == 'kl':\n",
    "                    W1 = F.softmax(W1, dim=-1)\n",
    "                    W2 = F.softmax(W2, dim=-1)\n",
    "                    log_W1 = torch.log(W1.clamp(min=1e-10))\n",
    "                    log_W2 = torch.log(W2.clamp(min=1e-10))\n",
    "\n",
    "                for t in range(T):\n",
    "                    k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "\n",
    "                    W1_leaf = W1.detach().requires_grad_(True)\n",
    "                    W2_leaf = W2.detach().requires_grad_(True)\n",
    "\n",
    "                    pred = self.memory_forward_deep(k_t.unsqueeze(1), W1_leaf, W2_leaf).squeeze(1)\n",
    "                    loss = self.get_loss(pred, v_t, x[:, t] if self.attentional_bias == 'huber' else None)\n",
    "                    grad1, grad2 = torch.autograd.grad(loss, [W1_leaf, W2_leaf])\n",
    "\n",
    "                    W1, log_W1 = self.apply_retention(W1, grad1, log_W1)\n",
    "                    W2, log_W2 = self.apply_retention(W2, grad2, log_W2)\n",
    "\n",
    "                    y_t = self.memory_forward_deep(q_t.unsqueeze(1), W1.detach(), W2.detach()).squeeze(1)\n",
    "                    outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "miras_l2 = MIRASLayer(n_embd, memory_type='deep', attentional_bias='l2', retention='l2').to(device)\n",
    "miras_lp = MIRASLayer(n_embd, memory_type='deep', attentional_bias='lp', retention='l2').to(device)\n",
    "miras_huber = MIRASLayer(n_embd, memory_type='deep', attentional_bias='huber', retention='l2').to(device)\n",
    "\n",
    "print(f\"MIRAS ‚Ñì‚ÇÇ: {miras_l2(test_emb).shape}\")\n",
    "print(f\"MIRAS ‚Ñì‚Çö: {miras_lp(test_emb).shape}\")\n",
    "print(f\"MIRAS Huber: {miras_huber(test_emb).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479f458",
   "metadata": {},
   "source": [
    "## Part 8: Complete MIRAS Language Model\n",
    "\n",
    "Now let's build a complete language model using our MIRAS layers. The architecture:\n",
    "1. Token embedding + Position embedding\n",
    "2. Stack of MIRAS layers\n",
    "3. Output projection to vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "874ea0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 642,180\n",
      "Model architecture: MIRASLanguageModel(\n",
      "  (token_embedding): Embedding(65, 128)\n",
      "  (position_embedding): Embedding(64, 128)\n",
      "  (layers): ModuleList(\n",
      "    (0-1): 2 x MIRASBlock(\n",
      "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (memory): MIRASLayer(\n",
      "        (kv_proj): KeyValueProjection(\n",
      "          (W_K): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (W_V): Linear(in_features=128, out_features=128, bias=False)\n",
      "          (W_Q): Linear(in_features=128, out_features=128, bias=False)\n",
      "        )\n",
      "        (ln): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (lm_head): Linear(in_features=128, out_features=65, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MIRASBlock(nn.Module):\n",
    "    \"\"\"A single MIRAS block: Memory layer + FFN, both with residual connections and LayerNorm.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, memory_type: str, attentional_bias: str, retention: str, ffn_mult: int = 4):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.memory = MIRASLayer(d_model, memory_type, attentional_bias, retention)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * ffn_mult),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * ffn_mult, d_model),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.memory(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MIRASLanguageModel(nn.Module):\n",
    "    \"\"\"Complete language model using MIRAS memory layers with FFN.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, d_model: int, n_layers: int,\n",
    "                 memory_type: str = 'deep',\n",
    "                 attentional_bias: str = 'l2',\n",
    "                 retention: str = 'l2',\n",
    "                 block_size: int = 128):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(block_size, d_model)\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            MIRASBlock(d_model, memory_type, attentional_bias, retention)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "\n",
    "        self.token_embedding.weight = self.lm_head.weight\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor, targets: Optional[torch.Tensor] = None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int, temperature: float = 1.0) -> torch.Tensor:\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "model = MIRASLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=n_embd,\n",
    "    n_layers=2,          # Reduced for memory (MIRAS uses more memory per layer)\n",
    "    memory_type='deep',\n",
    "    attentional_bias='l2',\n",
    "    retention='l2',\n",
    "    block_size=block_size\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model architecture: {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2afb53a",
   "metadata": {},
   "source": [
    "## Part 9: Training\n",
    "\n",
    "Let's train our MIRAS model on the Shakespeare dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc6552e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module, eval_iters: int = 50) -> dict:\n",
    "    \"\"\"Estimate loss on train and val sets.\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_miras(model: nn.Module, max_iters: int = 1000,\n",
    "                eval_interval: int = 100, learning_rate: float = 1e-3):\n",
    "    \"\"\"Training loop for MIRAS language model.\"\"\"\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss(model)\n",
    "            print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        xb, yb = get_batch('train')\n",
    "        _, loss = model(xb, yb)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    losses = estimate_loss(model)\n",
    "    print(f\"Final: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1258de53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2027, val loss 4.2023\n",
      "step 500: train loss 2.3533, val loss 2.3724\n",
      "step 1000: train loss 2.3915, val loss 2.4062\n",
      "step 1500: train loss 2.3558, val loss 2.3867\n",
      "step 2000: train loss 2.3163, val loss 2.3407\n",
      "step 2500: train loss 2.2865, val loss 2.3343\n",
      "step 3000: train loss 2.2714, val loss 2.3246\n",
      "step 3500: train loss 2.2677, val loss 2.3290\n",
      "step 4000: train loss 2.2502, val loss 2.3082\n",
      "step 4500: train loss 2.2372, val loss 2.3074\n",
      "Final: train loss 2.2355, val loss 2.2928\n"
     ]
    }
   ],
   "source": [
    "model = train_miras(model, max_iters=5000, eval_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b418a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      "\n",
      "Whe oelour h aravomaresh thtl! hus,\n",
      "ONUM:\n",
      "GOLIO:\n",
      "I'to grucos\n",
      "ETA:\n",
      "Mofult thoupy.\n",
      "\n",
      "\n",
      "\n",
      "akeatouge,\n",
      "INGHESS:\n",
      "I anrok'f le setheage\n",
      "Thavin, burelf trotthit mion! ort cr\n",
      "Ditherisspllucoman.\n",
      "\n",
      "\n",
      "CLIO: ma to ke.\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated = model.generate(context, max_new_tokens=200)\n",
    "print(\"Generated text:\")\n",
    "print(decode(generated[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db64b9",
   "metadata": {},
   "source": [
    "## Part 10: Experiments & Comparisons\n",
    "\n",
    "Let's compare different MIRAS configurations to understand the impact of each design choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63bd33ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(configs: list, max_iters: int = 300, eval_interval: int = 100):\n",
    "    \"\"\"Compare different MIRAS configurations.\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name, config in configs:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training: {name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        model = MIRASLanguageModel(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=n_embd,\n",
    "            n_layers=2,\n",
    "            block_size=block_size,\n",
    "            **config\n",
    "        ).to(device)\n",
    "\n",
    "        model = train_miras(model, max_iters=max_iters, eval_interval=eval_interval)\n",
    "\n",
    "        losses = estimate_loss(model)\n",
    "        results[name] = {\n",
    "            'train_loss': losses['train'].item(),\n",
    "            'val_loss': losses['val'].item(),\n",
    "            'params': sum(p.numel() for p in model.parameters())\n",
    "        }\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Summary\")\n",
    "    print(f\"{'='*50}\")\n",
    "    for name, res in results.items():\n",
    "        print(f\"{name:30s} | Train: {res['train_loss']:.4f} | Val: {res['val_loss']:.4f} | Params: {res['params']:,}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "configs = [\n",
    "    (\"Linear Memory + ‚Ñì‚ÇÇ\", {\"memory_type\": \"linear\", \"attentional_bias\": \"l2\", \"retention\": \"l2\"}),\n",
    "    (\"Deep Memory + ‚Ñì‚ÇÇ\", {\"memory_type\": \"deep\", \"attentional_bias\": \"l2\", \"retention\": \"l2\"}),\n",
    "    (\"Deep Memory + ‚Ñì‚ÇÉ\", {\"memory_type\": \"deep\", \"attentional_bias\": \"lp\", \"retention\": \"l2\"}),\n",
    "    (\"Deep Memory + Huber\", {\"memory_type\": \"deep\", \"attentional_bias\": \"huber\", \"retention\": \"l2\"}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c7604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run comparison (takes a few minutes)\n",
    "# results = compare_models(configs, max_iters=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aba8a7",
   "metadata": {},
   "source": [
    "## Part 11: Adding Momentum (Surprise Metric)\n",
    "\n",
    "The TITANS insight: **An event that violates expectations is more memorable.**\n",
    "\n",
    "**Surprise = gradient** of the loss w.r.t. input.\n",
    "\n",
    "**Problem**: Momentary surprise can miss important info after a big surprise.\n",
    "\n",
    "**Solution**: Track \"past surprise\" with momentum:\n",
    "\n",
    "$$S_t = \\eta_t S_{t-1} - \\theta_t \\nabla\\ell(M_{t-1}; k_t, v_t)$$\n",
    "$$M_t = (1 - \\alpha_t) M_{t-1} + S_t$$\n",
    "\n",
    "This is **gradient descent with momentum and weight decay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e985900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIRAS with momentum output: torch.Size([2, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "class MIRASLayerWithMomentum(nn.Module):\n",
    "    \"\"\"MIRAS layer with momentum (Titans-style surprise tracking).\n",
    "\n",
    "    Instead of directly applying gradients, we accumulate them with momentum\n",
    "    to track \"past surprise\" ‚Äî this helps remember important events that\n",
    "    happen after a big surprise.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, expansion: int = 4):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.kv_proj = KeyValueProjection(d, d)\n",
    "\n",
    "        self.W1_init = nn.Parameter(torch.randn(d, d * expansion) * 0.02)\n",
    "        self.W2_init = nn.Parameter(torch.randn(d * expansion, d) * 0.02)\n",
    "        self.ln = nn.LayerNorm(d)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(1) * 0.9)\n",
    "        self.eta = nn.Parameter(torch.ones(1) * 0.9)    # Momentum coefficient\n",
    "        self.theta = nn.Parameter(torch.ones(1) * 0.1)  # Gradient coefficient\n",
    "\n",
    "    def memory_forward(self, x: torch.Tensor, W1: torch.Tensor, W2: torch.Tensor) -> torch.Tensor:\n",
    "        h = F.gelu(x @ W2.transpose(-2, -1))\n",
    "        return x + self.ln(h @ W1.transpose(-2, -1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        k, v, q = self.kv_proj(x)\n",
    "        B, T, D = k.shape\n",
    "\n",
    "        W1 = self.W1_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "        W2 = self.W2_init.unsqueeze(0).expand(B, -1, -1).contiguous()\n",
    "\n",
    "        S1 = torch.zeros_like(W1)\n",
    "        S2 = torch.zeros_like(W2)\n",
    "\n",
    "        alpha = torch.sigmoid(self.alpha)\n",
    "        eta = torch.sigmoid(self.eta)\n",
    "        theta = F.softplus(self.theta)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            for t in range(T):\n",
    "                k_t, v_t, q_t = k[:, t], v[:, t], q[:, t]\n",
    "\n",
    "                W1_leaf = W1.detach().requires_grad_(True)\n",
    "                W2_leaf = W2.detach().requires_grad_(True)\n",
    "\n",
    "                pred = self.memory_forward(k_t.unsqueeze(1), W1_leaf, W2_leaf).squeeze(1)\n",
    "                loss = l2_loss(pred, v_t).sum()\n",
    "                grad1, grad2 = torch.autograd.grad(loss, [W1_leaf, W2_leaf])\n",
    "\n",
    "                S1 = eta * S1 - theta * grad1\n",
    "                S2 = eta * S2 - theta * grad2\n",
    "\n",
    "                W1 = (1 - alpha) * W1 + S1\n",
    "                W2 = (1 - alpha) * W2 + S2\n",
    "\n",
    "                y_t = self.memory_forward(q_t.unsqueeze(1), W1.detach(), W2.detach()).squeeze(1)\n",
    "                outputs.append(y_t)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "momentum_layer = MIRASLayerWithMomentum(n_embd).to(device)\n",
    "out = momentum_layer(test_emb)\n",
    "print(f\"MIRAS with momentum output: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4559a7",
   "metadata": {},
   "source": [
    "## Part 12: Parallelizable Training (Chunked)\n",
    "\n",
    "The recurrent updates above are sequential (slow on GPU). We can parallelize within **chunks**:\n",
    "\n",
    "1. Split sequence into chunks of size $b$ (e.g., 16 or 64)\n",
    "2. Within each chunk, use the **same** memory state for all gradients\n",
    "3. Accumulate gradients in parallel, then apply\n",
    "\n",
    "This trades off some accuracy for speed ‚Äî gradients within a chunk don't see each other's updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb2961b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked memory output: torch.Size([2, 8, 64])\n"
     ]
    }
   ],
   "source": [
    "class ChunkedLinearMemory(nn.Module):\n",
    "    \"\"\"Chunked parallel linear memory with Delta rule.\n",
    "\n",
    "    Within each chunk, we:\n",
    "    1. Use the same memory state M_0 for computing all gradients\n",
    "    2. Accumulate weighted gradients in parallel\n",
    "    3. Update memory at chunk boundaries\n",
    "\n",
    "    This is faster on GPUs at the cost of some approximation.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, chunk_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.chunk_size = chunk_size\n",
    "        self.kv_proj = KeyValueProjection(d, d)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.ones(1) * 0.9)\n",
    "        self.eta = nn.Parameter(torch.ones(1) * 0.1)\n",
    "\n",
    "    def process_chunk(self, M: torch.Tensor, k_chunk: torch.Tensor,\n",
    "                      v_chunk: torch.Tensor, q_chunk: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Process a chunk in parallel.\"\"\"\n",
    "        B, C, D = k_chunk.shape\n",
    "        alpha = torch.sigmoid(self.alpha)\n",
    "        eta = F.softplus(self.eta)\n",
    "\n",
    "        pred_all = torch.einsum('bde,bce->bcd', M, k_chunk)\n",
    "        errors = pred_all - v_chunk\n",
    "\n",
    "        decay_weights = alpha ** torch.arange(C, 0, -1, device=M.device).float()\n",
    "        decay_weights = decay_weights.view(1, -1, 1, 1)\n",
    "\n",
    "        grads = torch.einsum('bcd,bce->bcde', errors, k_chunk)\n",
    "        weighted_grads = (grads * decay_weights.squeeze(-1).unsqueeze(-1)).sum(dim=1)\n",
    "\n",
    "        M_new = (alpha ** C) * M - eta * weighted_grads\n",
    "\n",
    "        outputs = torch.einsum('bde,bce->bcd', M, q_chunk)\n",
    "\n",
    "        return M_new, outputs\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        k, v, q = self.kv_proj(x)\n",
    "        B, T, D = k.shape\n",
    "\n",
    "        M = torch.zeros(B, D, D, device=x.device)\n",
    "\n",
    "        n_chunks = (T + self.chunk_size - 1) // self.chunk_size\n",
    "        all_outputs = []\n",
    "\n",
    "        for i in range(n_chunks):\n",
    "            start = i * self.chunk_size\n",
    "            end = min(start + self.chunk_size, T)\n",
    "\n",
    "            k_chunk = k[:, start:end]\n",
    "            v_chunk = v[:, start:end]\n",
    "            q_chunk = q[:, start:end]\n",
    "\n",
    "            M, outputs = self.process_chunk(M, k_chunk, v_chunk, q_chunk)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "        return torch.cat(all_outputs, dim=1)\n",
    "\n",
    "chunked_mem = ChunkedLinearMemory(n_embd, chunk_size=8).to(device)\n",
    "out = chunked_mem(test_emb)\n",
    "print(f\"Chunked memory output: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660ca725",
   "metadata": {},
   "source": [
    "## Part 13: Key Insights & Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "We implemented the MIRAS framework from scratch, covering:\n",
    "\n",
    "| Component | Implementations |\n",
    "|-----------|----------------|\n",
    "| **Memory** | Linear (Hebbian, Delta), Deep MLP |\n",
    "| **Attentional Bias** | ‚Ñì‚ÇÇ, ‚Ñì‚Çö, Huber |\n",
    "| **Retention** | ‚Ñì‚ÇÇ decay, KL divergence, Elastic Net |\n",
    "| **Algorithm** | GD, GD+Momentum |\n",
    "\n",
    "### Key Equations\n",
    "\n",
    "**Hebbian (Linear Attention)**:\n",
    "$$M_t = \\alpha M_{t-1} + v_t k_t^\\top$$\n",
    "\n",
    "**Delta Rule**:\n",
    "$$M_t = \\alpha(I - \\eta k_t k_t^\\top) M_{t-1} + v_t k_t^\\top$$\n",
    "\n",
    "**Titans/MIRAS with Momentum**:\n",
    "$$S_t = \\eta_t S_{t-1} - \\theta_t \\nabla\\ell(M_{t-1}; x_t)$$\n",
    "$$M_t = (1 - \\alpha_t) M_{t-1} + S_t$$\n",
    "\n",
    "**Memora (KL Retention)**:\n",
    "$$W_t = c \\cdot \\text{Softmax}(\\alpha_t \\log(W_{t-1}) - \\eta_t \\nabla\\ell_2)$$\n",
    "\n",
    "### The MIRAS Unification\n",
    "\n",
    "The key insight is that **most sequence models are associative memory variants**:\n",
    "\n",
    "| Model | What it \"is\" |\n",
    "|-------|-------------|\n",
    "| Linear Attention | Hebbian memory + dot-product similarity |\n",
    "| DeltaNet | Linear memory + ‚Ñì‚ÇÇ similarity |\n",
    "| Mamba | Hebbian + structured decay |\n",
    "| Transformer | Non-parametric memory (stores all KV pairs) |\n",
    "| TTT-Linear/MLP | Trainable memory + ‚Ñì‚ÇÇ at test time |\n",
    "| Titans | Deep memory + momentum + ‚Ñì‚ÇÇ |\n",
    "| **Moneta** | Deep memory + ‚Ñì‚Çö + ‚Ñìq normalization |\n",
    "| **Yaad** | Deep memory + Huber (robust) |\n",
    "| **Memora** | Deep memory + KL retention |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Scale up**: Larger models, more data\n",
    "2. **Hybrid architectures**: Combine MIRAS with attention\n",
    "3. **Efficiency**: CUDA kernels for parallel scans\n",
    "4. **Novel biases**: Explore other loss functions\n",
    "5. **Applications**: Long-context tasks, memory-intensive problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c70ec",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "| Term | Definition |\n",
    "|------|------------|\n",
    "| **Attentional Bias** | Internal objective function defining \"similarity\" in memory |\n",
    "| **Retention Gate** | Mechanism balancing new learning vs. old retention |\n",
    "| **Momentary Surprise** | Gradient at current timestep |\n",
    "| **Past Surprise** | Momentum carrying surprise across tokens |\n",
    "| **Hebbian Rule** | Additive memory update: M += vk^T |\n",
    "| **Delta Rule** | Replacement memory update: removes old before adding new |\n",
    "| **MIRAS** | Memory, Attentional Bias, Retention, Algorithm, Sequence model |\n",
    "\n",
    "## References\n",
    "\n",
    "1. Behrouz et al. (2025). \"It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization\" (MIRAS paper)\n",
    "2. Behrouz et al. (2024). \"Titans: Learning to Memorize at Test Time\"\n",
    "3. Yang et al. (2024). \"Gated Delta Networks\"\n",
    "4. Sun et al. (2024). \"TTT: Learning to (learn at test time)\"\n",
    "5. Katharopoulos et al. (2020). \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
